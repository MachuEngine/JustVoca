# backend/app/api/study.py
from fastapi import APIRouter, UploadFile, File, Form, HTTPException, Depends
from pydantic import BaseModel
from sqlmodel import Session, select
import shutil
import os
import pandas as pd
import json
from typing import List, Optional, Dict
import random
from datetime import datetime, timedelta
import unicodedata  # [ì¶”ê°€] í•œê¸€ ìì†Œ ë¶„ë¦¬ ë°©ì§€ìš©

from app.core.database import get_session
from app.models import StudyProgress, StudyLog, User

router = APIRouter()

# --- [ê²½ë¡œ ì„¤ì •] ---
CURRENT_FILE_PATH = os.path.abspath(__file__)
BACKEND_DIR = os.path.dirname(os.path.dirname(os.path.dirname(CURRENT_FILE_PATH)))
EXCEL_PATH = os.path.join(BACKEND_DIR, "data", "vocab", "vocabulary.xlsx")
JSON_DATA_DIR = os.path.abspath(os.path.join(BACKEND_DIR, "..", "data", "index"))

class WordSchema(BaseModel):
    id: int
    level: str
    topic: str
    word: str
    pronunciation: str # ë°œìŒ í•„ë“œ
    meaning: str
    eng_meaning: str
    example: str
    audio_path: str 
    audio_example_path: str  # [ì¶”ê°€] ì˜ˆë¬¸ ì˜¤ë””ì˜¤ ê²½ë¡œ í•„ë“œ
    image_path: str  # ì´ë¯¸ì§€ ê²½ë¡œ í•„ë“œ

# ê¸°ë³¸ ë§¤í•‘ (ê³ ì • ì»¬ëŸ¼)
COLUMN_MAPPING = {
    "ì£¼ì œ": "topic", 
    "ë‹¨ì–´": "word", 
    "ë°œìŒ": "pronunciation", 
    "í•œê¸€ ëœ»": "meaning",     
    "ì˜ì–´ ëœ»": "eng_meaning", 
    "ì˜ˆë¬¸1": "example"
}

# ë ˆë²¨ë³„ JSON íŒŒì¼ ë§¤í•‘
LEVEL_JSON_MAP = {
    "ì´ˆê¸‰1": "level1.json",
    "ì´ˆê¸‰2": "level2.json",
    "ì¤‘ê¸‰1": "level3.json",
    "ì¤‘ê¸‰2": "level4.json",
    "ê³ ê¸‰1": "level5.json",
    "ê³ ê¸‰2": "level6.json",
}

def load_resource_map_by_id(level: str) -> Dict[str, Dict[str, str]]:
    """
    íŒŒì¼ëª…(ID)ì„ í‚¤ë¡œ í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ì˜ˆ: "Level1_1" -> {"image_path": "...", "audio_path": "..."})
    """
    json_filename = LEVEL_JSON_MAP.get(level, "level1.json")
    json_path = os.path.join(JSON_DATA_DIR, json_filename)
    
    id_resource_map = {}
    
    if os.path.exists(json_path):
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                items = data.get("items", [])
                for item in items:
                    resources = item.get("resources", {})
                    
                    # 1. ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ID ì¶”ì¶œ (ì˜ˆ: "audio/voca/Level1_1.wav" -> "Level1_1")
                    aud_file_raw = resources.get("audio_voca", {}).get("file", "")
                    if not aud_file_raw: continue
                    
                    # ê²½ë¡œì™€ í™•ì¥ìë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ íŒŒì¼ëª…ë§Œ IDë¡œ ì‚¬ìš©
                    file_id = os.path.splitext(os.path.basename(aud_file_raw))[0]
                    
                    # 2. ê²½ë¡œ ë³´ì •
                    img_file = resources.get("image", {}).get("file", "")
                    aud_file = aud_file_raw
                    aud_ex_file = resources.get("audio_ex", {}).get("file", "")
                    
                    if img_file and not img_file.startswith("/"): img_file = f"/assets/{img_file}"
                    if aud_file and not aud_file.startswith("/"): aud_file = f"/assets/{aud_file}"
                    if aud_ex_file and not aud_ex_file.startswith("/"): aud_ex_file = f"/assets/{aud_ex_file}"
                        
                    id_resource_map[file_id] = {
                        "image_path": img_file,
                        "audio_path": aud_file,
                        "audio_example_path": aud_ex_file
                    }
        except Exception as e:
            print(f"[Warning] Failed to load JSON by ID: {e}")
            
    return id_resource_map

# [í—¬í¼] ë¬¸ìì—´ ì •ê·œí™” (NFC: 'ã…'+'ã…' -> 'í•˜')
def normalize_text(text: str) -> str:
    if not text: return ""
    return unicodedata.normalize('NFC', str(text)).strip()

def load_resource_map(level: str) -> Dict[str, Dict[str, str]]:
    """
    í•´ë‹¹ ë ˆë²¨ì˜ JSON íŒŒì¼ì„ ì½ì–´ { "ë‹¨ì–´": { "image": "...", "audio": "..." } } í˜•íƒœì˜ ë§µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    (NFC ì •ê·œí™” ì ìš©)
    """
    json_filename = LEVEL_JSON_MAP.get(level, "level1.json")
    json_path = os.path.join(JSON_DATA_DIR, json_filename)
    
    resource_map = {}
    
    if os.path.exists(json_path):
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                items = data.get("items", [])
                for item in items:
                    # [ìˆ˜ì •] ì •ê·œí™” ì ìš©í•˜ì—¬ í‚¤ ì €ì¥
                    text_key = normalize_text(item.get("text", ""))
                    resources = item.get("resources", {})
                    
                    # ê²½ë¡œ ì¶”ì¶œ (JSON êµ¬ì¡°: resources > image > file)
                    img_file = resources.get("image", {}).get("file", "")
                    aud_file = resources.get("audio_voca", {}).get("file", "")
                    aud_ex_file = resources.get("audio_ex", {}).get("file", "") # [ì¶”ê°€] ì˜ˆë¬¸ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                    
                    # í”„ë¡ íŠ¸ì—”ë“œìš© ê²½ë¡œ ë³´ì • (/assets/ ì¶”ê°€)
                    if img_file and not img_file.startswith("/"):
                        img_file = f"/assets/{img_file}"
                    if aud_file and not aud_file.startswith("/"):
                        aud_file = f"/assets/{aud_file}"
                    if aud_ex_file and not aud_ex_file.startswith("/"):
                        aud_ex_file = f"/assets/{aud_ex_file}"
                        
                    resource_map[text_key] = {
                        "image_path": img_file,
                        "audio_path": aud_file,
                        "audio_example_path": aud_ex_file # [ì¶”ê°€]
                    }
        except Exception as e:
            print(f"[Warning] Failed to load JSON resources for {level}: {e}")
            
    return resource_map

@router.get("/current-progress")
async def get_current_progress(user_id: str, db: Session = Depends(get_session)):
    statement = select(StudyProgress).where(StudyProgress.user_id == user_id).order_by(StudyProgress.updated_at.desc())
    progress = db.exec(statement).first()
    if not progress:
        return {"level": "ì´ˆê¸‰1", "current_page": 1}
    return {"level": progress.level, "current_page": progress.current_page}

@router.get("/words", response_model=List[WordSchema])
async def get_words(level: str = "ì´ˆê¸‰1", user_id: Optional[str] = None, db: Session = Depends(get_session)):
    if not os.path.exists(EXCEL_PATH): return []

    try:
        current_page = 1
        if user_id:
            user = db.get(User, user_id)
            if not user:
                user = User(uid=user_id, name=user_id, role="student")
                db.add(user); db.commit()
            statement = select(StudyProgress).where(StudyProgress.user_id == user_id, StudyProgress.level == level)
            progress = db.exec(statement).first()
            if progress: current_page = progress.current_page

        xls = pd.ExcelFile(EXCEL_PATH, engine="openpyxl")
        target_sheet = next((s for s in xls.sheet_names if s.replace(" ", "") == level.replace(" ", "")), None)
        if not target_sheet: target_sheet = random.choice(xls.sheet_names)
        
        df = pd.read_excel(xls, sheet_name=target_sheet)
        
        # ì˜¤ë””ì˜¤ ì»¬ëŸ¼ ì²˜ë¦¬
        actual_cols = df.columns.tolist()
        audio_col = next((c for c in actual_cols if "Audio_Voca" in str(c) or "íŒŒì¼ ëª…" in str(c)), None)
        temp_mapping = COLUMN_MAPPING.copy()
        if audio_col: temp_mapping[audio_col] = "audio_path"
        df = df.rename(columns=temp_mapping)

        required_fields = list(COLUMN_MAPPING.values()) + ["audio_path"]
        for col in required_fields:
            if col not in df.columns: df[col] = ""
        df = df.fillna("")

        start_idx = (current_page - 1) * 10
        if start_idx >= len(df): start_idx = 0 
        paged_df = df.iloc[start_idx : start_idx + 10].copy()
        
        if 'level' not in paged_df.columns: paged_df['level'] = target_sheet
        if 'topic' not in paged_df.columns: paged_df['topic'] = "General"

        # JSON ë¦¬ì†ŒìŠ¤ ë¡œë“œ
        resource_map = load_resource_map_by_id(level) 
        data_list = []
        
        for idx, item in enumerate(paged_df.to_dict(orient="records")):
            # 2ë‹¨ê³„: ë§¤ì¹­ ê¸°ì¤€ì„ word_textì—ì„œ íŒŒì¼ëª…(audio_path)ìœ¼ë¡œ ë³€ê²½
            # ì—‘ì…€ì˜ 'íŒŒì¼ ëª…' í˜¹ì€ 'Audio_Voca' ì»¬ëŸ¼ ê°’ì´ ì´ë¯¸ audio_pathë¡œ ë§¤í•‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
            file_id = str(item.get('audio_path', '')).strip()
            
            # ID(íŒŒì¼ëª…)ë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ JSONì—ì„œ ê°€ì ¸ì˜¨ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            res = resource_map.get(file_id, {})
            
            # [ì•ˆì „ì¥ì¹˜] ë§Œì•½ IDë¡œ ëª» ì°¾ì•˜ì„ ê²½ìš°ì—ë§Œ ê¸°ì¡´ì²˜ëŸ¼ ë‹¨ì–´ í…ìŠ¤íŠ¸ë¡œ ì‹œë„ (ì„ íƒ ì‚¬í•­)
            if not res:
                word_text = normalize_text(item.get('word', ''))
                # ì´ ê²½ìš°ë¥¼ ìœ„í•´ load_resource_map_by_idì—ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§µë„ ê°™ì´ ê´€ë¦¬í•˜ë©´ ì¢‹ìœ¼ë‚˜,
                # íŒŒì¼ëª…ì´ í™•ì‹¤í•˜ë‹¤ë©´ file_idë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.

            data_list.append({
                "id": start_idx + idx + 1,
                "level": level,
                "topic": item.get('topic', 'General'),
                "word": normalize_text(item.get('word', '')), # ì—‘ì…€ì˜ ë‹¨ì–´
                "pronunciation": str(item.get('pronunciation', '')),
                "meaning": str(item.get('meaning', '')),
                "eng_meaning": str(item.get('eng_meaning', '')),
                "example": str(item.get('example', '')),
                # JSONì—ì„œ ì°¾ì€ ì˜¤ë””ì˜¤/ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì—‘ì…€ ê°’ ìœ ì§€
                "audio_path": res.get("audio_path", str(item.get('audio_path', ''))),
                "audio_example_path": res.get("audio_example_path", ""), 
                "image_path": res.get("image_path", "")
            })
        return data_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Data Load Error: {str(e)}")

@router.post("/evaluate")
async def evaluate_pronunciation(
    file: UploadFile = File(...), 
    word: str = Form(...),
    user_id: str = Form(...), 
    db: Session = Depends(get_session)
):
    upload_dir = "temp_uploads"
    os.makedirs(upload_dir, exist_ok=True)
    file_path = os.path.join(upload_dir, file.filename)
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    score = random.randint(75, 100)
    feedback = "ì°¸ ì˜í–ˆì–´ìš”!" if score > 85 else "ì¡°ê¸ˆë§Œ ë” í˜ë‚´ì„¸ìš”!"
    new_log = StudyLog(user_id=user_id, word=word, score=float(score), feedback=feedback)
    db.add(new_log)
    db.commit()

    return {"status": "success", "score": score, "feedback": feedback, "recognized_text": word}

@router.post("/complete")
async def complete_step(user_id: str = Form(...), level: str = Form(...), db: Session = Depends(get_session)):
    statement = select(StudyProgress).where(StudyProgress.user_id == user_id, StudyProgress.level == level)
    progress = db.exec(statement).first()
    if progress:
        progress.current_page += 1
        progress.updated_at = datetime.now()
        db.add(progress)
    else:
        new_progress = StudyProgress(user_id=user_id, level=level, current_page=2)
        db.add(new_progress)
    db.commit()
    return {"status": "success", "next_page": progress.current_page if progress else 2}

@router.get("/review-words")
async def get_review_words(user_id: str, db: Session = Depends(get_session)):
    statement = select(StudyLog).where(StudyLog.user_id == user_id).order_by(StudyLog.score.asc()).limit(5)
    logs = db.exec(statement).all()
    if not logs: return []
    review_list = []
    for log in logs:
        review_list.append({
            "id": log.id, "word": log.word, "pronunciation": "", "meaning": "", "eng_meaning": "", 
            "example": "", "audio_path": "", "image_path": "", "level": "", "topic": "Review"
        })
    return review_list

@router.get("/quiz")
async def get_quiz(level: str = "ì´ˆê¸‰1"):
    """
    [í€´ì¦ˆ ê¸°ëŠ¥]
    í•´ë‹¹ ë ˆë²¨ì˜ ì—‘ì…€ ë°ì´í„°ì—ì„œ ëœë¤í•˜ê²Œ 3ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(EXCEL_PATH):
        return []

    try:
        # 1. ì—‘ì…€ ë¡œë“œ
        xls = pd.ExcelFile(EXCEL_PATH, engine="openpyxl")
        target_sheet = next((s for s in xls.sheet_names if s.replace(" ", "") == level.replace(" ", "")), None)
        if not target_sheet:
            target_sheet = xls.sheet_names[0]
            
        df = pd.read_excel(xls, sheet_name=target_sheet).fillna("")
        
        # ì»¬ëŸ¼ ë§¤í•‘ (ë‹¨ì–´, ì˜ë¯¸ ì°¾ê¸°)
        df = df.rename(columns=COLUMN_MAPPING)
        
        # ë°ì´í„°ê°€ ì ìœ¼ë©´ ì „ì²´ ì‚¬ìš©, ë§ìœ¼ë©´ 3ê°œ ìƒ˜í”Œë§
        sample_size = min(3, len(df))
        quiz_samples = df.sample(n=sample_size).to_dict(orient="records")
        
        quizzes = []
        for i, item in enumerate(quiz_samples):
            correct_word = str(item.get('word', ''))
            description = str(item.get('meaning', item.get('ëœ»', '')))
            
            # ì˜¤ë‹µ ë³´ê¸° 3ê°œ ìƒì„± (ì •ë‹µì´ ì•„ë‹Œ ê²ƒ ì¤‘ì—ì„œ ëœë¤ ìƒ˜í”Œë§)
            distractors = df[df['word'] != correct_word]['word'].sample(n=3).tolist()
            options = distractors + [correct_word]
            random.shuffle(options)
            
            quizzes.append({
                "id": i + 1,
                "question": description, # ì˜ˆ: "ê°€ê¹ê²Œ ì˜¤ë˜ ì‚¬ê·„ ì‚¬ëŒ"
                "answer": correct_word,  # ì˜ˆ: "ì¹œêµ¬"
                "options": options       # ["ì¹œêµ¬", "í•™êµ", "ê³µë¶€", "ìš´ë™"]
            })
            
        return quizzes

    except Exception as e:
        print(f"Quiz generation error: {e}")
        return []

@router.get("/stats")
async def get_student_stats(user_id: str, db: Session = Depends(get_session)):
    """
    í•™ìƒ ê°œì¸ í•™ìŠµ í†µê³„ ì¡°íšŒ (ì‚¬ì–‘ì„œ ê¸°ë°˜)
    """
    # 1. í•™ìƒì˜ ëª¨ë“  í•™ìŠµ ë¡œê·¸ ì¡°íšŒ (ìµœì‹ ìˆœ)
    logs = db.exec(select(StudyLog).where(StudyLog.user_id == user_id).order_by(StudyLog.created_at.desc())).all()
    
    # 2. ì´ë²ˆ ì£¼(ìµœê·¼ 7ì¼) í•™ìŠµí•œ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
    now = datetime.now()
    seven_days_ago = now - timedelta(days=7)
    
    # ìµœê·¼ 7ì¼ ë‚´ì˜ ë¡œê·¸ë§Œ í•„í„°ë§
    weekly_logs = [log for log in logs if log.created_at >= seven_days_ago]
    # ì¤‘ë³µ ë‹¨ì–´ë¥¼ ì œì™¸í•˜ê³  ê°œìˆ˜ ì„¸ê¸° (set ì´ìš©)
    weekly_learned_count = len({log.word for log in weekly_logs})
    
    # 3. ì „ì²´ í‰ê·  ì •í™•ë„ ê³„ì‚°
    avg_accuracy = 0
    if logs:
        total_score = sum(log.score for log in logs)
        avg_accuracy = int(total_score / len(logs))
    
    # 4. ì—°ì† í•™ìŠµì¼(Streak) ê³„ì‚°
    streak = 0
    if logs:
        # ë¡œê·¸ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œí•˜ì—¬ ì¤‘ë³µ ì œê±° í›„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        dates = sorted(list({log.created_at.date() for log in logs}), reverse=True)
        today = now.date()
        
        # ê°€ì¥ ìµœê·¼ í•™ìŠµì¼ì´ ì˜¤ëŠ˜ì´ê±°ë‚˜ ì–´ì œì—¬ì•¼ ì—°ì† í•™ìŠµìœ¼ë¡œ ì¸ì •
        if dates and (today - dates[0]).days <= 1:
            streak = 1
            # ê³¼ê±° ë‚ ì§œë“¤ì„ ë¹„êµí•˜ë©° ì—°ì† ì—¬ë¶€ í™•ì¸
            for i in range(len(dates) - 1):
                if (dates[i] - dates[i+1]).days == 1:
                    streak += 1
                else:
                    break
    
    # 5. ì£¼ê°„ í•™ìŠµ ì¶”ì´ (ì›”~ì¼)
    # 0:ì›”ìš”ì¼, ... 6:ì¼ìš”ì¼
    weekly_trend = [0] * 7
    
    # ì´ë²ˆ ì£¼ì˜ ì‹œì‘ì¼(ì›”ìš”ì¼) êµ¬í•˜ê¸°
    start_of_week = now.date() - timedelta(days=now.weekday())
    
    for log in logs:
        log_date = log.created_at.date()
        # ë¡œê·¸ ë‚ ì§œê°€ ì´ë²ˆ ì£¼(ì›”~ì¼) ë²”ìœ„ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        if start_of_week <= log_date <= (start_of_week + timedelta(days=6)):
            day_idx = log_date.weekday() # 0(ì›”) ~ 6(ì¼)
            weekly_trend[day_idx] += 1
            
    # ê·¸ë˜í”„ í‘œí˜„ì„ ìœ„í•´ ê°€ì¥ ë§ì´ í•™ìŠµí•œ ë‚ ì„ 100%ë¡œ ì¡ê³  ì •ê·œí™”
    max_val = max(weekly_trend) if max(weekly_trend) > 0 else 1
    normalized_trend = [int((val / max_val) * 100) for val in weekly_trend]

    # 6. ìˆ™ë ¨ë„ (ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬)
    total_count = len(logs) if logs else 1
    high_count = len([l for l in logs if l.score >= 90])      # 90ì  ì´ìƒ: ì™„ì „ ì•”ê¸°
    mid_count = len([l for l in logs if 70 <= l.score < 90])  # 70~89ì : ë³µìŠµ í•„ìš”
    low_count = len([l for l in logs if l.score < 70])        # 70ì  ë¯¸ë§Œ: ë‹¤ì‹œ í•™ìŠµ
    
    proficiency = [
        {"label": "ì™„ì „ ì•”ê¸°", "value": int((high_count / total_count) * 100), "color": "bg-green-500"},
        {"label": "ë³µìŠµ í•„ìš”", "value": int((mid_count / total_count) * 100), "color": "bg-orange-400"},
        {"label": "ë‹¤ì‹œ í•™ìŠµ", "value": int((low_count / total_count) * 100), "color": "bg-red-400"},
    ]

    # ì‘ì› ë©”ì‹œì§€ ì„¤ì •
    message = "ì´ë²ˆ ì£¼ ëª©í‘œ ë‹¬ì„± ì¤‘! ğŸ”¥" if weekly_learned_count > 0 else "í•™ìŠµì„ ì‹œì‘í•´ë³´ì„¸ìš”! ğŸ’ª"

    return {
        "weeklyLearned": weekly_learned_count,
        "streak": streak,
        "accuracy": avg_accuracy,
        "weeklyTrend": normalized_trend,
        "proficiency": proficiency,
        "message": message
    }